\documentclass{article}

\usepackage{amsmath,amsfonts}
\usepackage{amsthm, amssymb}

\newtheorem{definition}{Definition}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand{\eps}{\varepsilon}
\newcommand{\tab}{\hspace{.5cm}}
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathcal N}
\renewcommand{\S}{\mathcal S}
\newcommand{\poly}{\operatorname{poly}}

\title{Data mining: lecture 11} %
\author{Edo liberty}
\date{\nonumber}

\begin{document} %
\maketitle 
\section{Nearest Neighbor search}
\begin{definition}

In the last class we discussed possible solutions for the nearest neighbor problem in low dimensions.
The problem is defined below.
{\bf Nearest Neighbor Search:} Given a set of points $\{x_1,\ldots,x_n\} \in \R^{d}$ 
preprocess them into a data structure
$X$ of size $O(n \poly(d))$ in time $O(n \poly(d))$ such that nearest neighbor queries can
be performed in logarithmic time. In other words, given a search point $q$ a radius $r$ and $X$ one can 
return an $x_i$ such the $||q - x_i || \le r$ or nothing if no such point exists.  The search for $x_i$ should
require $O(\poly(d,\log(n)))$ time.
\end{definition}
As we have seen, this problem is hard when the dimension $d$ is large.
It turns out that if we relax the problem a little bit, we can take a different tack at it.
\begin{definition}
{\bf Approximate Nearest Neighbor Search:} Given a set of points $\{x_1,\ldots,x_n\} \in \R^{d}$ 
preprocess them into a data structure
$X$ of size $O(n \poly(d))$ in time $O(n \poly(d))$ such that $\eps$-close nearest neighbor queries can
be performed in logarithmic time. In other words, given a search point $q$ a radius $r$ a constant $\eps$ and $X$ one can 
do one of two things. If there exists a point $x_i$ such that $||q - x_i || \le r$ then return possibly another point $x_j$ for which 
$||q - x_j || \le r(1+\eps)$. If for all $i$, $||q - x_i || > r(1+\eps)$, return no point.
The search should require $O(\poly(d,\log(n)))$ time.
\end{definition}


The idea is that of Local Sensitive Hashing (LSH). We define a family $\mathcal{H}$
of functions as $(r_1,r_2,p_1,p_2)$-sensitive if:
\begin{eqnarray*} 
|| x- y || < r_1 &\rightarrow& \Pr_{h \sim \mathcal{H}}(h(x)=h(y)) > p_1\\
|| x- y || > r_2 &\rightarrow& \Pr_{h \sim \mathcal{H}}(h(x)=h(y)) < p_2
\end{eqnarray*}
This is only meaningful if $r_1 < r_2$ and $p_1 > p_2$.
Which means that if $x$ and $y$ are ``close" then the probability that
they hash to the same value is at least something, but if the are further away
then it is smaller. Or, the probability of points being hashed to the same value 
decreases with their distance.


Let us assume such functions exist and give some intuition on how to use them.
First we concatenate $k$ different hash functions from $\mathcal{H}$ 
to construct a new hash function $g(x) = [h_1(x),\ldots,h_k(x)]$.
We choose $k$ such that $\Pr(g(x)=g(y)) \le 1/n$ if $||x-y|| > r_2$.
Using the $(r_1,r_2,p_1,p_2)$-sensitivity of $\mathcal{H}$ we will get that
if $||x-y|| < r_1$ then $\Pr(g(x)=g(y)) \ge 1/n^{\rho}$ for some $\rho<1$.

Now, if we generate $\ell = n^{\rho}$ different copies of $g$, $g_1,\ldots,g_\ell$,
and consider every $x$ in the data for which $g_i(x)=g_i(q)$ we will 
find every close point $x$ with constant probability and consider only $O(n^\rho)$ far points.

Let us make this statement more precise.
The preprocessing step is so.
\begin{enumerate}
\item $\rho \leftarrow \log(1/p_1)/\log(1/p_2)$
\item $\ell \leftarrow n^{\rho}$
\item $k \leftarrow \log(n)/log(1/p_2)$
\item for $\ell' \in \{1,\ldots,\ell\}$
\item \tab $g_{\ell'} \leftarrow [h_1(x),\ldots,h_k(x)]$
\item for $x \in X$
\item \tab for $\ell' \in \{1,\ldots,\ell\} $
\item \tab \tab add $x$ to $T_{\ell'}(g_{\ell'}(x))$
\end{enumerate}

The search stage is as follows:
\begin{enumerate}
\item $S \leftarrow \emptyset$
\item for $\ell' \in \{1,\ldots,\ell\} $
\item  \tab add $T_{\ell'}(g_{\ell'}(x)))$ to $S$
\item if $|S| \le 2n^{\rho}$
\item \tab for $x' \in S$
\item \tab  \tab if $||x' - q|| \le r_2$
\item \tab \tab \tab return  $x'$
\end{enumerate}



\begin{fact}
the number of points $x$ such that $||x-q|| \ge r_2$ and $x \in S$ is smaller that
$2\cdot n^{\rho}$ with probability at least $1/2$. 
\end{fact}
\begin{proof}
$x \in S$ is for some $\ell'$ we have $g_{\ell'}(q)  = g_{\ell'}(x)$ for $x$ such that $||x-q||>r_2$
this happens with probability $p_{2}^{log(n)/log(1/p_2)} = 1/n$. Thus, the expected total number of 
such points $x$ is $1$. Since we have $\ell = n^{\rho}$ different $g$ functions the total expected number of such
points is $n^{\rho}$. Due to the above and Markov's inequality $\Pr[|S| > 2n^{\rho}] \le \Pr[|S| > 2E[|S|]] \le 1/2$.   
\end{proof}

\begin{fact}
If $||x-q|| \le r_1$ then with constant probability $x \in S$
\end{fact}
\begin{proof}
By the $(r_1,r_2,p_1,p_2)$-sensitivity of $H$
\[
\Pr[g(x) = g(q)] \ge p_{1}^{k} = p_{1}^{\log(n)/\log(1/p_2)} = n^{-\log(1/p1)/\log(1/p_2)} = n^{-\rho}
\]
Since we repeat this $\ell = n^{\rho}$ times independently, we have that  $g_{\ell'}(x) \not = g_{\ell'}(q)$ for all 
$\ell'$ with probability at most $(1-n^{-\rho})^{n^{\rho}} < e^{-1}$ 
\end{proof}

Thus, both events happen with probability at least $1 - 1/2 - e^{-1} = const$.
We can duplicate the entire data structure $O(\log(1/\delta))$ time to achieve success probability $1-\delta$
in the cost of an $O(\log(1/\delta))$ factor in data storage and search time.
This means that the searching running time is $O(dn^{\rho})$.

\section{LSH functions}
\subsection{$\{0,1\}^d$  with the Hamming distance}
The humming distance between points are $x,y\in \{0,1\}^d$ is defined as 
the number of coordinates for which $x$ and $y$ defer. We claim that choosing a random 
coordinate from each vector is a local sensitive function and examine its parameters.   
\begin{fact}
let $\mathcal{H}$ be a family of $d$ functions for which $h_i(x) = x_i$.
Then, $\mathcal{H}$ is $(r,(1+\eps)r,1-\frac{r}{d},1-\frac{(1+\eps)r}{d})$-sensitive.
\end{fact}
\begin{fact}
If $r \le d/\log(n)$ then $\rho = \log(1/p_1)/\log(1/p_2) \le 1/(1+\eps)$
\end{fact}
\begin{proof}
See Fact 3 in \cite{GIM99}. Moreover, assuming $r \le d/\log(n)$ is harmless since we can always 
extend each vector by $d\log(n)$ zeros which does not change their distances and guaranties that  $r \le d/\log(n)$.
 \end{proof}


\begin{remark}
This results is also applicable to the Euclidian distance setting because it is possible
to map $\ell_{2}^{d}$ into $\ell_{1}^{O(d)}$ and also trivially possible to map $\ell_{1}^{d} = \{0,1\}^{O(d/\eps)}$ with distortion 
$\eps$ for bounded valued vectors. 
\end{remark}


Thus, the running time of $O(n^{\rho})$ is in fact $O(n^{1/(1+\eps)})$. In other words, to find a
the closest neighbor up to a factor of $2$ in this distance is possible while examining only $O(\sqrt{n})$ data points.
This, however, does not achieve the bound of $O(\poly(d,\log(n)))$.  

\subsection{Points in $\S^{d-1}$ and angular distance}
The set of unit length vectors in $\R^{d}$ is called the $d$ dimensional unit sphere and is denoted by  $\S^{d-1}$
(the power is $d-1$ to denote that it is actually a $d-1$ dimensional manifold. Do not be confused, the points are still in $\R^d$)
For these points, we can define the distance as the angle between the vectors $d(x,y) = cos^{-1}(x^{T}y)$.
We can thus define a hash function $h(x) = sign(u^{T}x)$ for a vector $u$ chosen uniformly at random from $\S^{d-1}$.
It is immediate to show that $h$ is local sensitive to the angular distance with parameters similar to the previous subsection.

\bibliographystyle{plain}
\bibliography{refs}
\end{document}














%%%%%%%%
