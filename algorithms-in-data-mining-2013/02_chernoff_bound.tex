\documentclass{article}
\usepackage{algorithms_in_data_mining}
\begin{document}
\lecture{2}{Probabilistic Inequalities}{Edo Liberty}



\begin{fact}[Markov's inequality]%
For any {\it positive} random variable $X$:
\begin{equation}
\Pr(X > t) \le \frac{E[X]}{t}
\end{equation}
\end{fact}

\begin{fact}[Chebyshev's inequality]%
For any random variable $X$
\begin{equation}
\Pr[|X-E[X]| > t] \le \frac{\sigma^2(X)}{t^2}
\end{equation}
\end{fact}



\begin{theorem}[Chernoff's bound]
Let $X_i$ be a set of {\bf independent} random variables such that $\E[X_i] = 0$ and $|X_i| \le 1$ almost surely.
Also define $\sigma_i^2 = \E[X_i^2]$ and $\sigma^2 = \sum_i \sigma_i^2$. Then:
\[
\Pr[ \sum_i X_i \ge t ] \le max(e^{-t^2/4\sigma^2} , e^{-t/2})
\]

\end{theorem}
\begin{proof}
\begin{eqnarray}
\Pr[ \sum_i X_i \ge t ] &=& \Pr[ \lambda \sum_i X_i \ge \lambda  t ]  \mbox{\;\; (for $\lambda \ge 0$)} \\
&= &\Pr[ e^{\lambda \sum_i X_i} \ge e^{\lambda  t} ]   \mbox{\;\; (because $e^x$ is monotone)} \\
&\le &\E[e^{\lambda \sum_i X_i}] /e^{\lambda  t} \mbox{\;\; (by Markov)} \\
&=& \Pi_i \E[e^{\lambda X_i}] /e^{\lambda  t} 
\end{eqnarray}

Now, for $x \in [0,1]$ we have that $e^x \le 1 + x + x^2$ so $\E[e^{\lambda X_i}] \le 1 + \E[\lambda X_i] + \lambda^2 \E[X_i^2] \le 1 + \lambda^2 \sigma^2_i$.
Now, since $1+x \le e^x$ we have that $1 + \lambda^2 \sigma^2_i \le e^{\lambda^2 \sigma_i^2}$. Combining the above we have that $\E[e^{\lambda X_i}] \le e^{\lambda^2 \sigma_i^2}$

\begin{eqnarray}
\Pi_i \E[e^{\lambda X_i}] /e^{\lambda  t} &\le& \Pi_i \E[e^{\lambda^2 \sigma_i^2}] /e^{\lambda  t}\\
&= & e^{\lambda^2 \sigma^2 - \lambda t}
\end{eqnarray}
Now, optimizing over $\lambda \in [0,1]$ we get that $\lambda = min(1,t/2\sigma^2)$ which completes the proof.


\subsection{Other Useful forms}
 {\bf Chernoff's inequality:} Let $X_1,\ldots,X_n$ be independent
$\{0,1\}$ valued random variables. Each $X_i$ takes the value $1$
with probability $p_i$ and $0$ else. Let $X = \sum_{i=1}^{n}X_i$ and
let $\mu = E[X] = \sum_{i=1}^{n}p_i$. Then:
\begin{eqnarray*}
\Pr[X > (1+\eps)\mu] &\le& e^{-\mu \eps^2/4}\\
\Pr[X < (1-\eps)\mu] &\le& e^{-\mu \eps^2/2}
\end{eqnarray*}
Or, using the union bound:
\[
\Pr[|X - \mu| > \eps\mu] \le 2e^{-\mu \eps^2/4}
\]


\end{proof}
\end{document}


%%%%%%%%
