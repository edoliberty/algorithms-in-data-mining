\input{environment.tex}
\begin{document} %
\lecture{6}{Fast random projection}{Edo Liberty}

We discussed in class the fact that random projection matrices cannot be made sparse in general.
That is because projecting sparse vectors and preserving their norm requires the projecting matrix is almost fully dense see also \cite{JelaniH2012} and \cite{KaneN12}.

But, the question is, can we actively make sure that $x$ is not sparse? If so, can we achieve a sparse random projection for non sparse vectors?
These two questions received a positive answer in the seminal work by Ailon and Chazelle \cite{AilonCh06}.
The results of \cite{AilonCh06} were improved and simplified over the years. See \cite{AilonL11} for the latest result and an overview.

In this lesson we will produce a very simple algorithm based on the ideas in \cite{AilonCh06}.
This algorithm will require a target dimension of $O(\log^2(n)/\eps^2)$ instead of $O(\log(n)/\eps^2)$ but will be much simpler to prove.

\subsection{Fast vector $\ell_4$ norm reduction}
The goal of this subsection is to devise a linear mapping which preserves vector's $\ell_2$ norms but reduces their $\ell_4$ norms with high probability.
This will work to our advantage because, intuitively, vectors whose $\ell_4$ norm is small cannot be too sparse.
For this we will need to learn what Hadamard matrices are.

Hadamard matrices are commonly used in coding theory and are conceptually
close for Fourier matrices. We assume for convenience that $d$ is a power of $2$ (otherwise we can pad out vectors with zeros).
The Walsh Hadamard transform of a vector $x \in \R^{d}$ is the
result of the matrix-vector multiplication $Hx$ where $H$ is a $d
\times d$ matrix whose entries are $H(i,j) = \frac{1}{\sqrt{d}}(-1)^{\langle
i,j\rangle}$. Here ${\langle i,j\rangle}$ means the dot product over
$F_2$ of the bit representation of $i$ and $j$ as binary vectors of
length $\log(d)$.
Another way to view this is to define Hadamard Matrices recursively.
\begin{equation*} %
H_{1} = \frac{1}{\sqrt{2}}\left(
          \begin{array}{rr}
            1 & 1 \\
            1 & -1\\
          \end{array}
        \right)
,\;\;
        H_{d} = \frac{1}{\sqrt{2}}\left(
          \begin{array}{r:r}
            H_{d/2} & H_{d/2} \\ \hdashline
            H_{d/2} & -H_{d/2}\\
          \end{array}
        \right)
\end{equation*} %
Here are a few interesting (and easy to show) facts about Hadamard matrices.
\begin{enumerate}
\item $H_d$ is a unitary matrix $\|Hx\| = \|x\|$ for any vector $x\in \R^d$.
%\item $H_{d}(i,j) \in \{ \frac{1}{\sqrt{d}},- \frac{1}{\sqrt{d}}\}$
\item Computing $x \mapsto Hx$ requires $O(d\log(d))$ operations.
\end{enumerate}


We also define a diagonal matrix $D$ to be such that $D(i,i) \in \{1,-1\}$ uniformly.
Clearly, we have that $\|HDx\|_2 = \|x\|_2$ since both $H$ and $D$ are isotropies.
Let us now bound $\|HDx\|_\infty$.
$(HDx)(1) = \sum_{i=1}^{d}H(1,i)D(i,i) x_i = \sum_{i=1}^{d}\frac{x_i}{\sqrt{d}}s_i$ where $s_i \in \{-1,1\}$ uniformly.
To bound this we recap Hoeffding's inequality.
\begin{fact}[Hoeffding's inequality]
Let $X_1,\ldots,X_n$ be independent random variables s.t. $X_i \in [a_i,b_i]$.
Let $X = \sum_{i=1}^{n} X_i$.
\begin{equation}
\Pr[|X - \E[X]| \ge t] \le 2e^{-\frac{2 t^2}{\sum_{i=1}^{n} (b_i -a_i)^2}}
\end{equation}
\end{fact}
Invoking Hoeffding's inequality and then the union bound we get that if $\|HDx\|_\infty \le \sqrt{\frac{c \log(n)}{d}}$ for all points $x$.
Remark, for this we assumed $\log(d) = O(\log(n))$ otherwise we should have had $\log(nd)$ in the bound. 
The situation, however, that the dimension is super polynomial in the number of points is unlikely. 
Usually it is common to have $n > d$.

\begin{lemma}
Let $x \in \R^d$ by such that $\|x\|=1$. Then:
\[
\|HDx\|^4_4 = O(log(n)/d)
\]
with probability at least $1-1/\poly(n)$
\end{lemma}
\begin{proof}
Let us define $y = HDx$ and $z_i$ = $y_i^2$. 
From the above we have that $z_i \le \frac{c \log(n)}{d} = \eta$ with probability at least $1-1/\poly(n)$.
The quantity $\|HDx\|^4_4 = \|y\|_{4}^{4} = \sum_{i}z_i^2$ is a convex function of the $z$ variables which is defined over a polytop $z_i \in [0,1]$ and $\sum_{i} z_i = 1$ (this is because $\|y\|_2^2 = 1$).
This means that its maximal value is obtained on an extreme point of this polytope. 
In other words, the point $z_1,\ldots,z_{1/\eta} = \eta$ and $z_{1/\eta+1},\ldots,z_{d} = 0$ or $z = [\eta,\eta,\ldots,\eta,\eta,0,0,0,\ldots,0,0,0]$.
Computing the value of the function in this point gives $\sum_{i}z_i^2 \le (1/\eta)\cdot (\eta^2) = \eta$. Recalling the $\eta = \frac{c \log(n)}{d}$ completes the proof.  
\end{proof}

\subsection{Sampling from vectors with low $\ell_4$ norms}
Here we prove a very simple fact. For vectors whose $\ell_4$ is low, dimensionally reduction can be obtained by sampling.


Let $y$ be a vector such that $\|y\|_2 = 1$. Let $z$ be a sampled version of $y$ such that $z_i = y_i/\sqrt{p}$ with probability $p$ and $0$ else. 
This is akin to sampling, in expectation, $d\cdot p$ coordinates from $y$ (and scaling them up by $1/\sqrt{p}$).
Note the $\E[\|z\|^2] = \E[\|y\|^2] = 1$ moreover:
\[
\Pr[|\|z\|^2 - 1| > \eps] = \Pr[|\sum z_i^2 - 1| > \eps] = \Pr[|\sum b_i y_i^2/p - 1| > \eps]
\]
Where $b_i$ are independent random indicator variables taking the $b_i = 1$ with probability $p$ and $b_i = 0$ else.
To apply chernoff's bound we must assert that $y_i^2/p \le 1$. Let's assume this for now and return to it later.
Applying Chernoff's bound we get
\[
\Pr[|\sum b_i y_i^2/p - 1| > \eps] \le e^{-\frac{c\eps^2}{\sigma^2}}
\]
where $\sigma^2 = \sum_{i} \E[(b_i y_i^2/p)^2] = \|y\|_{4}^{4}/p$.
Concluding that
\[
\Pr[|\|z\|^2 - 1| > \eps] \le e^{-\frac{cp\eps^2}{\|y\|_4^4}}
\]
This shows that the concentration of the sampling procedure really depends directly on the $\ell_4$ norm of the sampled vector.
If we plug in the bound on $\|y\|_4^4 = \|HDx\|_4^4$ from the previous section we get 
\[
\Pr[|\|z\|^2 - 1| > \eps] \le e^{-\frac{cp\eps d}{\log(n)}} \le \frac{1}{\poly(n)}
\]
For some $p \in O(\log^2(n)/d\eps^2)$. 

\subsection{Random Projection by Sampling}
Putting it all together we obtain the following.
\begin{lemma}
Define the following matrices
\begin{itemize}
\item $D$: A diagonal matrix such that $D_{i,i} \in \{+1,-1\}$ uniformly.
\item $H$: The $d\times d$ Walsh Hadamard Transform matrix.
\item $P$: A `sampling matrix' which contains each row of matrix $I_d\cdot \sqrt{p}$ with probability $p= c\log^2(n)/d\eps^2$.
\end{itemize}
Then, with at least constant probability the following holds.
\begin{enumerate}
\item The target dimension of the mapping is $k = c\log^2(n)/\eps^2$ (a factor $log(n)$ worse than optimal).
\item The mapping $x \mapsto PHDx$ is a $(1\pm\eps)$-distortion mapping for any set of $n$ points. 
That is, for any set $x_1,\ldots,x_n \in \R^d$ we have
\[
\|x_i -x_j\|(1-\eps) \le \|PHDx_i  - PHDx_j\| \le \|x_i -x_j\|(1+\eps)
\]
\item Storing $PHD$ requires at most $O(d + k\log(d))$ space.
\item Applying the mapping $x \mapsto PHDx$ requires at most $d\log(d)$ floating point operations.
\end{enumerate}
\end{lemma}


\bibliographystyle{unsrt}
\bibliography{AlgorithmsInDataMining.bib}

\end{document}














%%%%%%%%
