\documentclass{article}
\usepackage{algorithms_in_data_mining}
\begin{document} %
\lecture{12}{Approximate Nearest Neighbor Search}{Edo Liberty}

In this section we will review ideas from \cite{Charikar02} and \cite{GionisIM99}.
We define a family $\mathcal{H}$
of functions as $(r_1,r_2,p_1,p_2)$-sensitive if:
\begin{eqnarray*} 
|| x- y || < r_1 &\rightarrow& \Pr_{h \sim \mathcal{H}}(h(x)=h(y)) > p_1\\
|| x- y || > r_2 &\rightarrow& \Pr_{h \sim \mathcal{H}}(h(x)=h(y)) < p_2
\end{eqnarray*}
This is only meaningful if $r_1 < r_2$ and $p_1 > p_2$.
Which means that if $x$ and $y$ are ``close" then the probability that
they hash to the same value is at least something, but if they are further away
then it is smaller. Or, the probability of points being hashed to the same value 
decreases with their distance.


Let us assume such functions exist and give some intuition on how to use them.
First we concatenate $k$ different hash functions from $\mathcal{H}$ 
to construct a new hash function $g(x) = [h_1(x),\ldots,h_k(x)]$.
We choose $k$ such that $\Pr(g(x)=g(y)) \le 1/n$ if $||x-y|| > r_2$.
Using the $(r_1,r_2,p_1,p_2)$-sensitivity of $\mathcal{H}$ we will get that
if $||x-y|| < r_1$ then $\Pr(g(x)=g(y)) \ge 1/n^{\rho}$ for some $\rho<1$.

Now, if we generate $\ell = n^{\rho}$ different copies of $g$, $g_1,\ldots,g_\ell$,
and consider every $x$ in the data for which $g_i(x)=g_i(q)$ we will 
find every close point $x$ with constant probability and consider only $O(n^\rho)$ far points.

Let us make this statement more precise.
The preprocessing step is so.
\begin{enumerate}
\item $\rho \leftarrow \log(1/p_1)/\log(1/p_2)$
\item $\ell \leftarrow n^{\rho}$
\item $k \leftarrow \log(n)/log(1/p_2)$
\item for $\ell' \in \{1,\ldots,\ell\}$
\item \tab $g_{\ell'} \leftarrow [h_1(x),\ldots,h_k(x)]$
\item for $x \in X$
\item \tab for $\ell' \in \{1,\ldots,\ell\} $
\item \tab \tab add $x$ to $T_{\ell'}(g_{\ell'}(x))$
\end{enumerate}

The search stage is as follows:
\begin{enumerate}
\item $S \leftarrow \emptyset$
\item for $\ell' \in \{1,\ldots,\ell\} $
\item  \tab add $T_{\ell'}(g_{\ell'}(x)))$ to $S$
\item if $|S| \le 2n^{\rho}$
\item \tab for $x' \in S$
\item \tab  \tab if $||x' - q|| \le r_2$
\item \tab \tab \tab return  $x'$
\end{enumerate}



\begin{fact}
the number of points $x$ such that $||x-q|| \ge r_2$ and $x \in S$ is smaller that
$2\cdot n^{\rho}$ with probability at least $1/2$. 
\end{fact}
\begin{proof}
$x \in S$ is for some $\ell'$ we have $g_{\ell'}(q)  = g_{\ell'}(x)$ for $x$ such that $||x-q||>r_2$
this happens with probability $p_{2}^{log(n)/log(1/p_2)} = 1/n$. Thus, the expected total number of 
such points $x$ is $1$. Since we have $\ell = n^{\rho}$ different $g$ functions the total expected number of such
points is $n^{\rho}$. Due to the above and Markov's inequality $\Pr[|S| > 2n^{\rho}] \le \Pr[|S| > 2E[|S|]] \le 1/2$.   
\end{proof}

\begin{fact}
If $||x-q|| \le r_1$ then with constant probability $x \in S$
\end{fact}
\begin{proof}
By the $(r_1,r_2,p_1,p_2)$-sensitivity of $H$
\[
\Pr[g(x) = g(q)] \ge p_{1}^{k} = p_{1}^{\log(n)/\log(1/p_2)} = n^{-\log(1/p1)/\log(1/p_2)} = n^{-\rho}
\]
Since we repeat this $\ell = n^{\rho}$ times independently, we have that  $g_{\ell'}(x) \not = g_{\ell'}(q)$ for all 
$\ell'$ with probability at most $(1-n^{-\rho})^{n^{\rho}} < e^{-1}$ 
\end{proof}

Thus, both events happen with probability at least $1 - 1/2 - e^{-1} = const$.
We can duplicate the entire data structure $O(\log(1/\delta))$ time to achieve success probability $1-\delta$
in the cost of an $O(\log(1/\delta))$ factor in data storage and search time.
This means that the searching running time is $O(dn^{\rho})$.

\section{LSH functions}
\subsection{$\{0,1\}^d$  with the Hamming distance}
The humming distance between points which are $x,y\in \{0,1\}^d$ is defined as 
the number of coordinates for which $x$ and $y$ defer. We claim that choosing a random 
coordinate from each vector is a local sensitive function and examine its parameters.   
\begin{fact}
let $\mathcal{H}$ be a family of $d$ functions for which $h_i(x) = x_i$.
Then, $\mathcal{H}$ is $(r,(1+\eps)r,1-\frac{r}{d},1-\frac{(1+\eps)r}{d})$-sensitive.
\end{fact}
\begin{fact}
If $r \le d/\log(n)$ then $\rho = \log(1/p_1)/\log(1/p_2) \le 1/(1+\eps)$
\end{fact}
\begin{proof}
See Fact 3 in \cite{GionisIM99}. Moreover, assuming $r \le d/\log(n)$ is harmless since we can always 
extend each vector by $d\log(n)$ zeros which does not change their distances and guaranties that  $r \le d/\log(n)$.
 \end{proof}


\begin{remark}
This results is also applicable to the Euclidean distance setting because it is possible
to map $\ell_{2}^{d}$ into $\ell_{1}^{O(d)}$ and also trivially possible to map $\ell_{1}^{d} = \{0,1\}^{O(d/\eps)}$ with distortion 
$\eps$ for bounded valued vectors. 
\end{remark}


Thus, the running time of $O(n^{\rho})$ is in fact $O(n^{1/(1+\eps)})$. In other words, to find a
the closest neighbor up to a factor of $2$ in this distance is possible while examining only $O(\sqrt{n})$ data points.
This, however, does not achieve the bound of $O(\poly(d,\log(n)))$.  

\subsection{Searching with similarities}
Note that in the above we never used the fact that the distance function is a metric. 
Indeed, it is possible to search though items as long as we can produce a local sensitive hashing.
In \cite{Charikar02} Charikar defined Local sensitive hashing as:
\[
\Pr_{h}[h(x)=h(y)] = sim(x,y)
\]
For example, let $x$ and $y$ be sets of items. Their set similarity can be defined as $\frac{| x \cap y|}{|x \cup y|}$.
Here we can use a famous trick. We will map $h(x) \rightarrow \arg \min_{x_i \in x} g(x_i)$ when $g$ is a random permutation over
the entire universe or a random function into $[0,1]$ for example. The reason this holds true is because 
the minimal value of $g$ in $|x \cup y|$ might accidentally be also in $| x \cap y|$ but 
since the distribution is uniform, the probability of this event is $\frac{| x \cap y|}{|x \cup y|}$.
 
 \subsection{LSH for points in $\Sph^{d-1}$}
The set of unit length vectors in $\R^{d}$ is called the $d$ dimensional unit sphere and is denoted by  $\Sph^{d-1}$
(the power is $d-1$ to denote that it is actually a $d-1$ dimensional manifold. Do not be confused, the points are still in $\R^d$)
For these points, we can define the distance as the angle between the vectors $d(x,y) = cos^{-1}(x^{T}y)$.
We can thus define a hash function $h(x) = sign(u^{T}x)$ for a vector $u$ chosen uniformly at random from $\Sph^{d-1}$.
It is immediate to show that $h$ is local sensitive to the angular distance with parameters similar to the previous subsection.

\bibliographystyle{plain}
\bibliography{algorithms_in_data_mining}
\end{document}














%%%%%%%%
