\documentclass{article}
\usepackage{AlgorithmsInDataMining}
\begin{document}

\lecture{8}{Home Assignment, Due Dec 31st}{Edo Liberty}

\section{Randomized meta-algorithms}
\subsection*{setup}
In this question we assume the common case where we have an input $x \in X$  
and we wish to approximate a function $f:X \rightarrow \R^+$ (i.e. $\forall x\;\;f(x) \ge 0$).
For that we have a black box randomized algorithm $A:X\rightarrow \R^+$ such that $\E[A(x)] = f(x)$.
The questions ask you to designing meta algorithms using $A$ as a black box. 
\subsection*{question}
\begin{enumerate}
\item Show that
\[
\Pr[A(x) \ge 3f(x)] \le \frac{1}{3}
\]
\item Assume that for all $x$ we have that $\Var[A(x)] \le c\cdot [f(x)]^2$.
Describe an algorithm $B_2$ such that for any two constants $\eps,\delta > 0$:
\[
\Pr[|B_2(x) - f(x)| \ge \eps f(x)] \le \delta
\]
\item Assume that $\Pr[|A(x) - f(x) | \le t] \ge \frac{1}{2}+\eta$ for some fixed value $\eta > 0$.
In words, the algorithm gets an additive approximation $t$ with probability slightly better than $1/2$.
(Here we do not assume anything on the variance of $A(x)$).
Design and algorithm $B_3$ such that for any prescribed $\delta >0$
\[
\Pr[|B_3(x) - f(x) | \le t] \ge 1 - \delta
\]
That means the algorithm achieves the same additive approximation with probability arbitrary close to one.
\end{enumerate}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\median}{\operatorname{median}}

\subsection*{answers}
\begin{enumerate}
\item This is an application of Markov's inequality which is adequate since $A(x) \ge 0$
\[
\Pr[A(x) > t] \le \frac{\E[A(x)]}{t}
\]
Setting $t = 3f(x) = 3\E[A(x)]$ completes the claim. 
\item Let $B_2$ be defined as 
\[
B_2(x) = \frac{1}{s}\sum_{i=1}^{s}A_{i}(x) \mbox{\;\; for \;\;} s=\frac{c}{\eps^2\delta}
\]
where $A_{i}$ are independent executions of $A$.
Since $A_{i}(x)$ are independent we have that $\E[B_2(x)] = f(x)$ and $\Var[B_2(x)] = \Var[A(x)]/s$.
Substituting this into Chebyshev's inequality we get
\[
\Pr[|B_2(x) - f(x)| \ge \eps f(x)] \le \frac{\Var[A(x)]/s}{\eps^2 f^2(x)} \le  \frac{c/s}{\eps^2} \le \delta
\] 
\item Here we can define $B_3$ as
\[
B_3(x) = \median \{A_{i}(x) | i \in [s]\} \mbox{\;\; for \;\;} s \ge \frac{2 \log(1/\delta)}{\eta^2}
\]
where $A_{i}$ are again independent executions of $A$.
Let us define $z_i =1$ if $|A_i(x) - f(x) | \le t$ and zero else.
It is easy to see that if $\sum_{i=1}^{s}z_i \ge s/2$ then the algorithms succeeds.
This is because there are at least $s/2$ values $A_i(x)$ in the interval $(f(x)-t,f(x)+t)$. 
Therefore, the median must be one of those values and must also lie in $(f(x)-t,f(x)+t)$. 
We now use Chernoff's inequality for $\sum_{i=1}^{s}z_i$ and use the fact that $\mu = \E[\sum_{i=1}^{s}z_i] \ge s(1/2+\eta)$
\[
\Pr[\sum_{i=1}^{s}z_i < s/2] = \Pr[\sum_{i=1}^{s}z_i - s(1/2+\eta) < -s\eta] \le \Pr[\sum_{i=1}^{s}z_i - \mu < - s\eta] \le e^{-\frac{s^2\eta^2}{4\mu}} \le e^{-\frac{s\eta^2}{2}} \le \delta 
\]
We conclude that since $\Pr[\sum_{i=1}^{s}z_i < s/2] \le \delta$ then $\Pr[\sum_{i=1}^{s}z_i \ge s/2] \ge 1-\delta$ which is the algorithm's success probability.
\end{enumerate}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Set intersections} 
\subsection*{setup}
We have a universe of $N$ items $A = \{a_1,\ldots, a_N\}$
and $m$ subsets $S_i \subset A$, $i \in \{1,\ldots,m\}$.
We assume that given a set $S_i$ we can iterate over its elements one by one.
The exercise will deal with approximating the size of different unions of these sets.  
Here you are tasked with designing an algorithm. 
Your algorithm is allowed to preprocess the sets $S$ in and produce data structures ($preprocess(S)$)
It should then be able to take as input a set of indexed $I \subset \{1,\ldots,m\}$ and produce
an $\eps$ approximation to $|\cup_{i \in I}S_i|$ with probability at least $1-\delta$ ($estimateUnionSize(I)$).
The aim is to create an algorithm which runs in time $o(\sum_{i \in I}|S_i|)$ and requires $o(|\cup_{i \in I}S_i|)$ space. 
That means that simply iterating through the lists and keeping items in a hash lookup table is not an adequate solution. 

\begin{enumerate}
\item Describe $preprocess(S)$ which is the preparatory stage of the algorithm and results in our choice of data structures.
\item Describe $estimateUnionSize(I)$ which return an $\eps$ approximation to $|\cup_{i \in I}S_i|$ with probability $1-\delta$.
\item Prove your algorithm's correctness.
\item What is the space usage of your data structures?
\item What is the runtime complexity of $estimateUnionSize(I)$?
\end{enumerate}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection*{answers}

\begin{enumerate}
\item We first choose $s \ge 8/\eps^2\delta$ hash functions $h_i : a \rightarrow [0,1]$ uniformly.
For each set $S_i$ of the $m$ sets we compute for each hash function $h_j$ its minimal value over the elements of $S_i$.
Storing these concludes the preprocessing step which requires $O(s\sum_{i=1}^{m}|S_i|)$ hash evaluations and $O(sm)$ storage.
Note that here we assume that the number of elements in the universe $n$ is such that $log(n)$ is small enough to be treated as a constant.
Otherwise, the hash functions must contain $\Omega(\log(n))$ bits which would give an $O(s\log(n)\sum_{i=1}^{m}|S_i|)$ running time
and $O(sm\log(n))$ storage.

\item Once $I$ is received, we compute the $s$ minimal values over the sets $S_i$ s.t. $i \in I$ for each hash function.
This is done simply by taking the minimal values from the ones already computed in the preprocessing step. 
Denoting by $x_j$ this minimal value (for hash function $h_j$) we return $\frac{1}{\frac{1}{s}\sum_{i=1}^{s}x_i}$.
\item The proof is identical to a proof given in class (and the class notes) so I will not repeat it here.
The main statement is that the reciprocal to the mean of $s \ge 8/\eps^2\delta$ minimal hash value over a set of $n'$ objects is
an $\eps$ approximation to $n'$ with probability at least $1-\delta$.
The algorithm clearly computes these minimal values for the set $\cup_{i \in I}S_i$ which completes the proof.    
\item The amount of space is as stated before $O(sm) = O(8m/\eps^2\delta)$ or $O(8m\log(n)/\eps^2\delta)$ depending
on the computational model.
\item Given that all $sm$ minimal hash values are given in an array with $O(1)$ access time, the amount of time
to compute the approximated size of $\cup_{i \in I}S_i$ is $O(s|I|)$.
\end{enumerate}

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Weak random projections}
\subsection*{setup}
In this question we will construct a simple and weak proof of the Johnson-Lindenstrauss lemma. 
Given two vectors $x,y \in\R^{d}$ we will find two new vectors $x',y'\in \R^{k}$ such that from $x'$ and
$y'$ we could approximate the value of $||x-y||$. The idea is to
define $k$ vectors $r_i \in \R^{d}$ such that each $r_i(j)$ takes a
value in $\{+1,-1\}$ uniformly at random. Setting $x'(i) =
r_{i}^{T}x$ and $y'(i) = r_{i}^{T}y$ the questions will lead you through arguing that
$\frac{1}{k}||x' -y'||_{2}^{2} \approx ||x-y||_{2}^{2}$.

\subsection*{questions}
\begin{enumerate}
\item Let $z = x-y$, and $z' = x' - y'$. Show that $z'(\ell) =
r_{\ell}^{T}z$ for any index $\ell \in [1,\ldots,k]$.

\item Show that $E[\frac{1}{k}||z'||_{2}^{2}] = E[(z'(\ell))^2] = ||z||_{2}^{2}$.
\item Show that
\[
\var[(z'(\ell))^2] \le 4||z||_{2}^{4}.
\]
Hint: for any vector $w$ we have $||w||_4 \le ||w||_2$.
\item From 3 (even if you did not manage to show it) claim that
\[
\var[\frac{1}{k}||z'||_{2}^{2}] \le 4||z||_{2}^{4}/k.
\]
\item Use 3 and Chebyshev's inequality do obtain a value for $k$
for which:
\[
(1-\eps)||x-y||_{2}^{2} \le \frac{1}{k}||x' -y'||_{2}^{2} \le
(1+\eps)||x-y||_{2}^{2}
\]
with probability at least $1-\delta$.
\end{enumerate}

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection*{answers}
\begin{enumerate}
\item This is a consequence of the linearity of the operator. 
\[
z'(\ell) = x'(\ell) - y'(\ell) = r_{\ell}^{T}x - r_{\ell}^{T}y = r_{\ell}^{T}(x-y) = r_{\ell}^{T}z 
\]
\item Since $||z'||_{2}^{2} = \sum_{i=1}^{k}z'(i)^2$ and since $z'(i)$ are identically distributed we have that 
$\E[\frac{1}{k}||z'||_{2}^{2}] = \E[\frac{1}{k}\sum_{i=1}^{k}z'(i)^2] = \E[(z'(\ell))^2]$.
Now we compute $\E[(z'(\ell))^2]$.
\begin{eqnarray}
\E[(z'(\ell))^2] &=& \E[(\sum_{i=1}^{d} r_\ell(i) z(i))(\sum_{j=1}^{d} r_\ell(j) z(j))] \\
&=& \E[\sum_{i=1}^{d} \sum_{j=1}^{d} r_\ell(i) r_\ell(j)z(i) z(j)] \\
&=& \sum_{i=1}^{d} \sum_{j=1}^{d} \E[r_\ell(i) r_\ell(j)]z(i) z(j) \\
&=& \sum_{i=1}^{d} z(i)^2  = \|z\|^2
\end{eqnarray}
The double summation was reduced to a single sum since $\E[r_\ell(i)r_\ell(j)] = 0$ if $i \ne j$.
Also, if $i=j$ we have that $\E[r_\ell(i)r_\ell(j)]z(i)z(j) = z(i)^2$
\item To compute $\var[(z'(\ell))^2]$ we start with computing $\E[(z'(\ell))^4]$.
\begin{eqnarray*}
\E[(z'(\ell))^4] &=& \E[(\sum_{i=1}^{d} r_\ell(i) z(i))(\sum_{j=1}^{d} r_\ell(j) z(j))(\sum_{k=1}^{d} r_\ell(k) z(k))(\sum_{m=1}^{d} r_\ell(m) z(m))] \\
&=& \E[\sum_{i=1}^{d}\sum_{j=1}^{d}\sum_{k=1}^{d}\sum_{m=1}^{d}r_\ell(i)r_\ell(j)r_\ell(k)r_\ell(m)z(i)z(j)z(k)z(m)] \\
&=& \sum_{i=1}^{d}\sum_{j=1}^{d}\sum_{k=1}^{d}\sum_{m=1}^{d} \E[r_\ell(i)r_\ell(j)r_\ell(k)r_\ell]z(i)z(j)z(k)z(m)\\
&=& \sum_{i=1}^{d}x(i)^4 + {4 \choose 2} \sum_{i<j}z(i)^2 z(j)^2
\end{eqnarray*}
The last transition requires an explanation. The expectation of $r_\ell(i)r_\ell(j)r_\ell(k)r_\ell$ when the power of one of 
the terms $r_\ell(i)$ is odd is zero. Thus, we are only left with terms of the form $x(i)^4$ and $x(i)^2 x(j)^2$.
The coefficient of $x(i)^4$ is $1$ since there is only one what to obtain it. The coefficient of $x(i)^2 x(j)^2$ is ${4 \choose 2}$
since two of the indexes should be $i$ and the two others $j$. There are ${4 \choose 2} = 6$ to get it.
In what comes next we use the fact that:
\[
\sum_{i<j}z(i)^2 z(j)^2 = [\sum_{i=1}^{d}\sum_{j=1}^{d}z(i)^2 z(j)^2 -  \sum_{i=1}^{d}z(i)^4]/2
\]
Picking up where we left off:
\begin{eqnarray*}
\E[(z'(\ell))^4] &=& \sum_{i=1}^{d}x(i)^4 + 6 \sum_{i<j}z(i)^2 z(j)^2\\
&=& \sum_{i=1}^{d}x(i)^4 + 3[\sum_{i=1}^{d}\sum_{j=1}^{d}z(i)^2 z(j)^2 -  \sum_{i=1}^{d}z(i)^4] \\
&=& 3\|z\|_{2}^{4} - 2\|z\|_4^2 
\end{eqnarray*}
Finally we have that 
\begin{eqnarray*}
\var( z'(\ell)^2) &=& \E[(z'(\ell))^4]  -\E[(z'(\ell))^2]^2 \\
&=& 3\|z\|_{2}^{4} - 2\|z\|_4^2 - (\|z\|_{2}^{2})^2 = 2(\|x\|_2^4 - \|x\|_4^4) \le 2\|x\|_2^4 
\end{eqnarray*}
\item Since $z'(\ell)$ are independent variables we have that 
\[
\var[\frac{1}{k}\|z'\|^2] = \var[\frac{1}{k} \sum_{\ell=1}^{k}z'(\ell)^2] =  \frac{1}{k^2}\sum_{\ell=1}^{k}\var[z'(\ell)^2] = \frac{1}{k}\var[z'(\ell)^2] \le 2\|x\|_2^4/k
\]
\item From Chebishev's inequality we have that 
\[
\Pr[|\frac{1}{k}\|z'\|^2 - \E[\frac{1}{k}\|z'\|^2]| \ge t] \le \frac{\var[\frac{1}{k}\|z'\|^2]}{t^2}
\]
Substituting $\E[\frac{1}{k}\|z'\|^2] = \|z\|^2$, $t = \eps\|z\|^2$ and $\var[\frac{1}{k}\|z'\|^2] \le 2\|x\|_2^4/k$ we get:
\[
\Pr[|\frac{1}{k}\|z'\|^2 - \|z\|]| \ge \eps \|z\|] \le \frac{2\|x\|_2^4/k}{\eps^2\|z\|^4} = \frac{2}{k\eps^2} 
\]
By setting $k \ge \frac{2}{\eps^2 \delta}$ we get that $\Pr[|\frac{1}{k}\|z'\|^2 - \|z\|]| \ge \eps \|z\|] \le \delta$ which means that
$\|z\|(1-\eps)\le \frac{1}{k}\|z'\|^2 \le \|z\|(1+\eps)$ with probability at least $1-\delta$.
\end{enumerate}









\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{SVD and the power method}
\subsection*{setup}

Here we will prove some basic facts about singular values, matrices, and the power method.
For the reminder of the question we assume $A \in \R^{m \times n}$ is an arbitrary matrix.
For convenience and w.l.o.g. assume $m \le n$ and denote by $\sigma_1 \ge \ldots \sigma_m \ge 0$
the singular values of $A$.
Define the numeric rank of a matrix $\rho(A)$ to be $\rho(A) = \|A\|^{2}_{F}/\|A\|^2_2$. $\rho(A)$ is a smoothed
version of the algebraic rank $rank(A)$. It is always true that $1\le \rho(A) \le Rank(A) \le \min(m,n)$.
If $\rho(A) \le 1 +\eps$ for a sufficiently small $\eps$ the matrix is ``close'' to being of rank $1$.

\subsection*{question}

\begin{enumerate}
\item Let $P \in \R^{m \times m}$ and $Q \in \R^{n \times n}$ be unitary matrices.
Show that $\|PAQ\|_{F} = \|A\|_{F}$.
Hint, begin with the case where one of the matrices $P$ or $Q$ are the identity matrix.
\item Using the above show that for any matrix $A$ we have that 
\[
\|A\|_{F} = \sqrt{\sum_{i=1}^{m}\sigma_{i}^{2}}.
\]
It might help you to show that $\|A\|^{2}_{F} = tr(AA^T)$ where $tr(\cdot)$ stands for the matrix trace.
\item Give an expression to the numeric rank of $A$ only in terms of its singular values $\sigma_i$. 
\item Express the numerical rank of $(AA^T)^{k}A$ only in terms of  $\sigma_i$.
\item Assume that the matrix $A$ is such that $\sigma_2/\sigma_1 \le \eta$ for some $\eta < 1$.
Use your expressions from above to find $k$ such that $\rho((AA^T)^{k}A)) \le  1+ \eps$.
How does this relate to the the Power Method for computing the largest singular value and vectors of $A$?
\end{enumerate}



\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection*{answers}
\begin{enumerate}
\item As the hint suggests, let us first show that $\|AQ\|_{F} = \|A\|_{F}$ for any matrix $A$ and unitary matrix $Q$.
\[
\|AQ\|^2_{F} = \sum_{i=1}^{n} \|(AQ)_i\|^2_2 = \sum_{i=1}^{n} \|A_iQ\|^2_2 = \sum_{i=1}^{n} \|A_i\|^2_2 = \|A\|^2_F
\]
Here $(AQ)_i$ denoted the $i$'th columns of $AQ$ and $A_i$ denoted the $i$'th column of $A$. Repeating this argument for the left side completes the claim.
\item As we saw in class, if we compute the SVD of $A$ we get $A = USV^T$ where $U$ and $V$ are unitary and $S$ is diagonal such that $S_{i,i} = \sigma_i$.
Using the above we get that $\|A\|_F = \|S\|_F = \sqrt{\sum_{i=1}^{m}\sigma_{i}^{2}}$.
\item The numeric rank of $A$ is defined as $\|A\|_F^2/\|A\|_2^2$. Since $\|A\|_2^2 = \sigma_1^2$ and $\|A\|_F^2 = \sum_{i=1}^{m}\sigma_{i}^{2}$ we have
\[
\rho(A) = \frac{\|A\|_F^2}{\|A\|_2^2} = \sum_{i=1}^{m}(\frac{\sigma_{i}}{\sigma_1})^{2}
\]
\item In term of the SVD of $A$ we have that $(AA^T)^{k}A = US^{2k+1}V^T$. 
Therefore the singular values of $(AA^T)^{k}A$ are equal to $\sigma_i^{2k+1}$.
Using the equation above we get that
\[
\rho((AA^T)^{k}A)  = \sum_{i=1}^{m}(\frac{\sigma_{i}}{\sigma_1})^{4k+2}
\]
\item Rewriting the expression above we get
\[
\sum_{i=1}^{m}(\frac{\sigma_{i}}{\sigma_1})^{4k+2} = (\frac{\sigma_{1}}{\sigma_1})^{4k+2} + \sum_{i=2}^{m}(\frac{\sigma_{i}}{\sigma_1})^{4k+2} \le 1+ m\eta^{4k+2}
\]
Which gives us that $\rho((AA^T)^{k}A) \le 1+\eps$ when $k \ge \log(m/\eps)/4\log(\eta) + O(1)$.
This gives another explanation for the success of the power method. 
The reason is that even for relatively small values of $k$ the matrix $(AA^T)^{k}A$ is essentially rank $1$. 
This means that the direction of $(AA^T)^{k}Ax$ is independent of $x$.
It only depends on the left (and only) singular vector of $(AA^T)^{k}A$.
The power method computes $(AA^T)^{k}Ax$ iteratively and returns the resulting vector (normalized) which is the left singular vector of $A$ corresponding to the top singular value. 

\end{enumerate}





\end{document}
















%%%%%%%%
