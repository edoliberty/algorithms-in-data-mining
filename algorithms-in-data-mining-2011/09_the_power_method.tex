\documentclass{article}
\usepackage{algorithms_in_data_mining}

\begin{document} %

\lecture{9}{The power method}{Edo Liberty}

We give a simple algorithm for computing the Singular Value Decomposition of a matrix $A \in \R^{m \times n}$.
We start by computing the first singular value $\sigma_1$ and left and right singular vectors $u_1$ and $v_1$ of $A$,
for which $min_{i<j}\log(\sigma_i/\sigma_j) \ge \lambda$:
\begin{enumerate}
\item Generate $x_0$ such that $x_0(i) \sim \N(0,1)$.
\item $s \leftarrow  \log(4\log(2n/\delta)/\eps\delta)/2\lambda$ 
\item for $i$ in $[1,\ldots,s]$:
\item \tab $x_i \leftarrow A^{T}Ax_{i-1}$
\item $v_1 \leftarrow x_i/\norm{x_i}$  
\item $\sigma_1 \leftarrow \norm{Av_1}$
\item $u_1 \leftarrow Av_1/\sigma_1$ 
\item return $(\sigma_1,u_1,v_1)$ 
\end{enumerate}
Let us prove the correctness of this algorithm.
First, write each vector $x_i$ as a linear combination of the right singular values of $A$ i.e. $x_i = \sum_{j} \alpha^{i}_{j}v_j$. 
From the fact that $v_j$ are the eigenvectors of $A^{T}A$ corresponding to eigenvalues $\sigma^{2}_j$ 
we get that $\alpha^{i}_{j}= \alpha^{i-1}_{j}\sigma^{2}_{j}$.
Thus, $\alpha^{s}_{j} = \alpha^{0}_{j}\sigma^{2s}_{j}$. Looking at the ratio between the coefficients of $v_1$ and $v_i$ for $x_s$
we get that:
 \[
 \frac{|<x_s,v_1>|}{|<x_s,v_i>|} = \frac{|\alpha^{0}_{1}|}{|\alpha^{0}_{i}|}\left(\frac{\sigma_1}{\sigma_i}\right)^{2s}
\]
Demanding that the error in the estimation of $\sigma_1$ is less than $\eps$ gives the requirement on $s$.
\begin{eqnarray}
\frac{|\alpha^{0}_{1}|}{|\alpha^{0}_{i}|}\left(\frac{\sigma_1}{\sigma_i}\right)^{2s} &\ge& \frac{n}{\eps}\\
s &\ge& \frac{\log(n|\alpha^{0}_i|/\eps|\alpha^{0}|_1)}{2\log(\sigma_1/\sigma_i)}
\end{eqnarray}
From the two-stability of the gaussian distribution we have that $\alpha^{0}_i \sim \N(0,1)$. 
Therefore, $\Pr[\alpha^{0}_i > t] \le e^{-t^2}$ which gives that with probability at least $1-\delta/2$ we have for
all $i$, $|\alpha^{0}_i | \le \sqrt{\log(2n/\delta)}$. Also, $\Pr[|\alpha^{0}_1 | \le \delta/4 ] \le \delta/2$ (this is because 
$\Pr[|z| < t] \le max_{r}\Psi_{z}(r)\cdot2t$ for any distribution and the normal distribution function at zero takes it maximal value which is less than $2$) 
Thus, with probability at least $1-\delta$ we have that for all $i$, $\frac{|\alpha^{0}_{1}|}{|\alpha^{0}_{i}|} \le \frac{\sqrt{\log(2n/\delta)}}{\delta/4}$.
Combining all of the above we get that it is sufficient to set $s = \log(4n\log(2n/\delta)/\eps\delta)/2\lambda = O(\log(n/\eps\delta)/\lambda)$
in order to get $\eps$ precision with probability at least $1-\delta$.

We now describe how to extend this to a full SVD of $A$. Since we have computed $(\sigma_1,u_1,v_1)$, we can repeat this
procedure for $A - \sigma_{1}u_{1}v_{1}^{T} = \sum_{i=2}^{n}{\sigma_{i}u_{i}v_{i}^{T}}$. The top singular value and vectors of which are $(\sigma_2,u_2,v_2)$.
Thus, computing the rank-k approximation of $A$ requires $O(mnks)  = O(mnk\log(n/\eps\delta))/\lambda)$ operations. 
This is because computing $A^{T}Ax$ requires $O(mn)$ operations and
for each of the first $k$ singular values and vectors this is performed $s$ times. 

The main problem with this algorithm is that its running time is heavily influenced by the value of $\lambda$.
Other variants of this algorithm are much less sensitive to the value of this parameter, but are out of the scope of this class. 


\end{document}













%%%%%%%%
