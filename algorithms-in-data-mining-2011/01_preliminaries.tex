\documentclass{article}
\usepackage{algorithms_in_data_mining}

\begin{document} %

\lecture{1}{Preliminaries}{Edo Liberty}

\section{Preliminaries}
A variable $X$ is a random variable if it assumes different values
according to probability distribution. For example, $X$ can 
denote the outcome of a three sided die throw and $X$ taking the values $x
= 1,2,3$ with equal probabilities. The
expectation of $X$ is the sum over the possible values times the
probability of the events.
\begin{equation}
E[X] = \sum_{x=1}^{3}x \Pr(X = x)=
1\frac{1}{3}+2\frac{1}{3}+3\frac{1}{3} = 2
\end{equation}

Another example is a continuous variable $Y$ taking the values
$[0,1]$ uniformly. Meaning that the probability of $Y$ being in the
interval $[t,t + dt]$ is exactly $dt$. And so the expectation of $Y$
is:
\begin{equation}
E[Y] = \int_{t=0}^{1}dt \cdot t = \frac{1}{2}t^2|_{0}^{1} = 1/2
\end{equation}


\subsection{Dependance and Independence}
A variable $X$ is said to be {\it dependent} on $Y$ if given the
value of $Y$ changes the probability distribution of $X$. For
example. Assume the variable $X$ takes the value $1$ if $Y$ takes a
value of less than $1/3$ and the values $2$ or $3$ with equal
probably otherwise ($1/2$ each).

Clearly, the probability of $X$ assuming each of its values is still
$1/3$. however, if we know that $Y$ is $0.7234$ the probability of
$X$ assuming the value $1$ is zero.

This is denoted by $E(X | Y)$ (read: expectation of $X$ given $Y$).
\begin{eqnarray}
E(X | Y) = \sum_{x=1}^{3} x \Pr(X = x | Y \le 1/3) = 1\cdot 1\\
E(X | Y) = \sum_{x=1}^{3} x \Pr(X = x | Y > 1/3) = 1\cdot 0 + 2
\frac{1}{2} + 3\frac{1}{2}  = 2.5
\end{eqnarray}
Note that $E(X | Y)$ is a function of $y$!! $E(X | Y) = 1$ for $y
\in [0,1/3]$ and $E(X | Y) = 2.5$ for $y \in (1/3,1]$.

\begin{definition}
Two variables are said to be {\it Independent} if:
\[
\forall y,\;\;E[ X | Y = y] = E[X].
\]
They are {\it dependent} otherwise.
\end{definition}


\begin{fact}
For any two random variables (even if they are dependent):
\begin{equation}
E_{Y}E_{X}[ X | Y] = E[X]
\end{equation}
\end{fact}
Checking this for our example:
\begin{equation}
E_{Y}E_{X} [ X | Y] = \int_{y=0}^{1} E_{X}[X|Y = y] =
\int_{y=0}^{1/3}1dy + \int_{y=1/3}^{1}2.5dy = \frac{1}{3} \cdot 1 +
\frac{2}{3}\cdot 2.5 = 2
\end{equation}

\begin{fact}[Linearity of expectation 1]%
For any random variable and any constant $\alpha$:
$$E[\alpha X] = \alpha E[X]$$
\end{fact}

\begin{fact}[Linearity of expectation 2]%
For any random variable and any constant $\alpha$:
$E[\alpha X] = \alpha E[X]$
For any two random variables (even if they are dependent):
\begin{equation}
E_{X,Y}[X+Y] = E[X] + E[Y]
\end{equation}
\end{fact}
Given the previous fact we can convince ourselves that this is true.
$E_{X,Y}[x+y] = E_{Y}E_{X}[X+Y] = E_{Y} [X|Y] + E_{Y} [Y] = E_X[X] +
E_Y[Y]$.

\begin{fact}[Markov's inequality]%
For any {\it positive} random variable $X$:
\begin{equation}
\Pr(X > t) \le \frac{E[X]}{t}
\end{equation}
\end{fact}


\begin{definition}
The variance of a random variable $x$ is:
\begin{equation}
\sigma^2[X] = E[(X-E[X])^2] = E[X^2] - E[X]^2
\end{equation}
\end{definition}

\begin{fact}[Chebyshev's inequality]%
For any random variable $X$
\begin{equation}
\Pr[|X-E[X]| > t] \le \frac{\sigma^2(X)}{t^2}
\end{equation}
\end{fact}

\end{document}


%%%%%%%%
