\documentclass{article}
\usepackage{algorithms_in_data_mining}

\begin{document} %

\lecture{6}{Random Projections}{Edo Liberty}

We will give a simple proof of the following, rather amazing, fact. Every set of $n$ points 
in a Euclidian space (say in dimension $d$) can be embedded into the Euclidian space of 
dimension $O(\log(n)/\eps^2)$ such that all pairwise distances are preserved up distortion $1\pm \eps$.

\section*{Random projection}
We will argue that a certain distribution over the choice of a matrix $R \in \R^{k \times d}$ gives that:
\begin{equation}
\label{e1}
\forall x \in \R^{d} \;\; \Pr\left[ \left| ||\frac{1}{\sqrt{k}}Rx|| - ||x|| \right| > \eps||x|| \right] \le \frac{1}{n^2} 
\end{equation}
Before we show this distribution and show that Equation~\ref{e1} holds for it, let us first see
that this will gives the opening statement. 

Consider a set of $n$ points $x_1,\ldots, x_n$ in Euclidian space $\R^d$. Embedding these points
into a lower dimension while preserving all distances between
them up to distortion $1\pm \eps$ means approximately preserving the norms of all 
${n \choose 2}$ vectors $x_i - x_j$. Assuming Equation~\ref{e1} holds and using the union 
bound, this property will fail to hold for at least one $x_i - x_j$ pair with probability at most ${n \choose 2}\frac{1}{n^2} \le 1/2$.
Which means that all ${n \choose 2}$ point distances are preserved up to distortion $\eps$ with probability at least $1/2$.


\section{I.i.d Gaussian distribution}
We consider the distribution of matrices $R$ such that each $R(i,j)$ is drawn independently from  a
normal distribution with mean zero and variance $1$, $R(i,j) \sim \N(0,1)$. We will show that for this
distribution Equation~\ref{e1} holds for some $k \in O(\log(n)/\eps^2)$.

First consider the random variable $z = \sum_{i=1}^{d}r(i)x(i)$ where $r(i) \sim \N(0,1)$. 
To understand how the variable $z$ distributes we recall the two-stability of the
normal distribution. Namely, if $z_3 = z_2 + z_1$ and $z_1 \sim \N(\mu_1,\sigma^{2}_{1})$ and $z_2 \sim \N(\mu_2,\sigma^{2}_{2})$ then, $z_3 \sim \N(\mu_1 + \mu_2,\sigma^{2}_{1} + \sigma^{2}_{2})$.
In our case,  $r(i)x(i) \sim \N(0,x^{2}_{i})$ and therefore, $z = \sum_{i=1}^{d}r(i)x(i) \sim \N(0,\sum_{i=1}^{d}x^{2}_{i}) = \N(0,||x||^{2}) =  ||x||\cdot\N(0,1)$. Now, note that each element in the vector $Rx$ distributes exactly like $z$.
Defining $k$ identical copies of $z$, $z_1,\ldots,z_k$,
We get that $||\frac{1}{\sqrt{k}}Rx||$ distributes exactly like:
\[
||\frac{1}{\sqrt{k}}Rx|| \sim \sqrt{\frac{1}{k}\sum_{i=1}^{k} z^{2}_{i}} = \sim ||x||\sqrt{\frac{1}{k}\sum_{i=1}^{k} y^{2}_{i}}
\]
where $y_i \sim \N(0,1)$.
Thus, proving Equation~\ref{e1} reduces to showing that:
\begin{equation}
\Pr\left[ \left| \sqrt{\frac{1}{k}\sum_{i=1}^{k} y^{2}_{i}} - 1 \right| > \eps \right] \le \frac{1}{n^2} 
\end{equation}

It is now straight forward to show since the sum of $k$ squared normal variables is a very known distribution called chi-square with $k$ degrees of freedom. ($\chi^2_k)$.
More accurately, it is defined by $\chi^2_k = \sum_{i=1}^{k} y^{2}_{i}$ where $y_i \sim \N(0,1)$ which is exactly what we have.
Since $\chi^2_k$ is a sum of independent random variables, due to the central limit theorem, $\chi^2_k$ converges to a normally distributed quantity as $k$ grows. We will use here a slightly different property: 
$\sqrt{\chi^2_k} \sim_{k \rightarrow \infty} \N(\sqrt{k},1/2)$. Somewhat sloppily, we will assume that $k$ is large enough so that 
assuming $\sqrt{\chi^2_k} \sim \N(\sqrt{k-1/2},1/2) \approx \N(\sqrt{k},1/2)$ is harmless. I that case, $ \sqrt{\frac{1}{k}\sum_{i=1}^{k} y^{2}_{i}} \sim \N(1,\frac{1}{2k})$ and $ \sqrt{\frac{1}{k}\sum_{i=1}^{k} y^{2}_{i}} -1\sim \N(0,\frac{1}{2k})$.
Thus, we only need to show that for a random variable $Z \sim \sqrt{2k}\left[\sqrt{\frac{1}{k}\sum_{i=1}^{k} y^{2}_{i}} -1\right] \sim,\sqrt{2k}\N(0,\frac{1}{2k})  \sim \N(0,1)$ it holds that  
\begin{equation}
\Pr\left[ | Z | > \eps \sqrt{2k}\right] \le \frac{1}{n^2} 
\end{equation}
We now use a bound on the error function: $\int_{t=\eps \sqrt{2k}}^{\infty} \frac{1}{\sqrt{2\Pi}}e^{-t^2/2}dt = erf(\eps \sqrt{2k}) \le e^{-\eps^2k}$. Since $\Pr[Z  > \eps \sqrt{2k}] = \Pr[Z  < -\eps \sqrt{2k}] $we demand that $e^{-\eps^2k} \le \frac{1}{2n^2}$. This yields the bound $k \ge \frac{2\log(n)+1}{\eps^2}$.



\end{document}














%%%%%%%%
