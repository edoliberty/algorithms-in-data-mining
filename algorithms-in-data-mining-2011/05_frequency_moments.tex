\documentclass{article}
\usepackage{algorithms_in_data_mining}

%\newcommand{\tab}{\hspace{.5cm}}

\begin{document} %

\lecture{5}{Estimating Frequency Moments}{Edo Liberty}

Assume we have a stream $A$, of length $N$ which is composed of $m$ 
different types of items $a_1,\ldots,a_m$ each of which repeats itself $n_1,\ldots,n_m$ times (in arbitrary order)
We define the frequency moments $f_k$ as:
\[
f_k = \sum_{i=1}^{m} n_{i}^{k}
\]

Our aim to to process the stream in one element at a time and attain an $(\epsilon,\delta)$-approximation.
This means, that our estimate is up to multiplicative factor $(1\pm\epsilon)$ with probability at least $1-\delta$.  
Note that $f_0$ is the number of distinct elements in the stream $m$ and that $f_1$ is the number of elements $N$.
$f_2$ is also an important quantity which represents how "skewed" the distribution of the elements in stream is.


Let's first assume that we know $N$ in advance. This is not necessary and we will fix it later. But for now, it makes our
analysis simpler.

Let us first define a random variable $X$. We choose an index $q \in [1,\ldots,N]$ uniformly at random.
Let $a$ be the element in place $q$ in the stream, i.e. $a=A_q$. Define by $r$ the number of times $a$ appears in 
the stream after location $q$, including. In other words $r = |\{i | A_i = a \;,\; i\ge q\}|$.
We define $X$:
\[
X = N(r^k - (r-1)^k)
\]

We claim that $E[X] = f_k$. Let us define the variable $e_{i,j}$ which indicates the event that 
the index $q$ is such that $A_q = a_i$ and $a_i$ appears exactly $j$ times after the location $q$.
Note that the events $e_{i,j}$ are disjoint and that if $e_{i,j}$ happens than $r$ takes the value $j$. 
Therefore, $X = \sum_{i,j} e_{i,j}N(j^k - (j-1)^k)$. Moreover, $\Pr[e_{i,j}] = \frac{n_{i}}{N}\frac{1}{n_i} = \frac{1}{N}$     
since the probability of choosing $a_i$ is $\frac{n_i}{N}$ and given that this happens the probability of each index
(out of the locations of $a_i$) is equal to $\frac{1}{n_i}$.
\begin{eqnarray*}
E[X] &=& \sum_{i,j} E[e_{i,j}N(j^k - (j-1)^k)]\\
&=&\sum_{i=1}^{m}\sum_{j=1}^{n_i} \Pr[e_{i,j}]N(j^k - (j-1)^k)\\
&=&\sum_{i=1}^{m}\sum_{j=1}^{n_i} (j^k - (j-1)^k)\\
&=&\sum_{i=1}^{m} n_{i}^{k} = f_k \;\; . 
\end{eqnarray*}
It is somewhat complicated and tedious to compute the variance of $X$. Citing from the paper \cite{}
we use the fact that 
\[
Var[X] \le km^{1-1/k}f_{k}^{2} \; .
\] 


We define $Y$ as the mean of $s$ different copies of $X$, $Y = \frac{1}{s}\sum_{i=1}^{s}X_i$.
Clearly, $E[Y] = E[X] = f_k$ and $Var[Y] \le Var[X]/s =  km^{1-1/k}f_{k}^{2}/s$.
Using Chebyshev's inequality we have that 
\[
\Pr[|Y - f_k| > \eps f_k] \le \frac{Var[Y]}{\eps^2 f_{k}^{2}} \le \frac{km^{1-1/k}}{\eps^2 s} \le \delta.
\]
where the last inequality holds if $ s \ge \frac{km^{1-1/k}}{\eps^2 \delta}$.

\section*{Estimating $f_0$}
This bound is not the most efficient algorithm for approximating the 
zero'th frequency moment (which is the number of distinct elements, $m$).
Here we will describe a more efficient algorithm which is a merging of ideas from \cite{} and \cite{}.

First, assume a hash function $h: a \rightarrow [0,1]$ uniformly.
Let us define a random variable $X = min_{i}h(a_i)$.
Intuitively, $X$ should be roughly $1/m$ and therefore $1/X$ should be a fair estimate of $m$.
This is almost true. In what comes next we make this into an exact statement.


Let us first compute the expectation of $X$. The distribution function $f_X$ of the $X$
is $f_X(x) = m(1-x)^{m-1}$. This is because, we have $m$ different choices for the minimal 
element and for every value it takes, $x$, all the rest $m-1$ values
need to be higher than it (w.p $(1-x)^{m-1}$). Therefore, u:
\begin{eqnarray*}
E[X] &=& \int_{0}^{1} x m (1-x)^{m-1}dx \\
&=& \int_{0}^{1} (1- y) m y^{m-1}dy \\
&=& \int_{0}^{1} m y^{m-1}dy - \int_{0}^{1} m y^{m}dy\\
&=& 1- \frac{m}{m+1} = \frac{1}{m+1}
\end{eqnarray*}
This is after the substitution $y = 1-x$. We now compute the variance of $X$.
For that we first compute $E[X^2]$.
\begin{eqnarray*}
E[X^2] &=& \int_{0}^{1} x^2 m (1-x)^{m-1}dx \\
&=& \int_{0}^{1} (1- y)^2 m y^{m-1}dy \\
&=& \int_{0}^{1} m y^{m-1}dy - \int_{0}^{1} 2 m y^{m}dy + \int_{0}^{1}  m y^{m+1}dy\\
&=& 1- \frac{2m}{m+1} + \frac{m}{m+2} \le \frac{3}{(m+1)^2}
\end{eqnarray*}
Thus, the standard deviation of $\sigma(X)$ is in the same order of magnitude as its expectation $E[X]$.
To reduce this ratio we again define $Y = \frac{1}{s}\sum_{i=1}^{s}X_i$ for which $E[Y] = \frac{1}{m+1}$.
and $Var[Y] \le \frac{2}{s(m+1)^2}$.

Using Chebyshev's inequality we get that 
\[
\Pr[|Y - \frac{1}{m+1}| \ge \frac{\eps/2}{m+1}] \le \frac{8}{\eps^2 s} \le \delta
\] 
if $s \ge \frac{8}{\eps^2 \delta}$. Therefore, multiplying this procedure $\frac{8}{\eps^2 \delta}$ different 
hash function and taking their mean minimal value guaranties that with probability at least $1-\delta$ we have
$\frac{1}{m+1}(1-\eps/2) \le Y \le \frac{1}{m+1}(1+\eps/2)$.
In other words: $(m+1)\frac{1}{1+\eps/2} \le \frac{1}{Y} \le (m+1)\frac{1}{1-\eps/2}$.
But, since $\frac{1}{1-\eps/2} \le 1+ \eps$ and $1- \eps \le \frac{1}{1+\eps/2}$ we get the desired results
that $(m+1)(1-\eps) \le \frac{1}{Y} \le (m+1)(1+\eps)$



\section*{Estimating $f_2$}

We will give here a better estimator of $f_2$.
Assume a hash function $h:a\rightarrow \{-1,1\}$ with probability $1/2$ each.
Define $Z = \sum_{i=1}^{N}h(A_i) = \sum_{i=1}^{m}n_i h(a_i)$.

Consider the variable $X = Z^2$. As usual, we will begin with computing the expectation and variance of $X$.
\begin{eqnarray*}
E[X] &=& E[Z^2] = E[\sum_{i=1}^{m}n_i h(a_i)^2]\\
&=& E[(\sum_{i=1}^{m}n_i h(a_i))(\sum_{i'=1}^{m}n_{i'} h(a_{i'}))]\\
&=& \sum_{i=1}^{m}\sum_{i'=1}^{m}n_i n_{i'} E[h(a_{i}) h(a_{i'})]\\
&=& \sum_{i=1}^{m} n_{i}^{2} = f_2
\end{eqnarray*}
 
Similarly,
\begin{eqnarray*}
E[X^2] &=& E[Z^4] = \sum_{i=1}^{m} n_{i}^{4} + 6\sum_{1\le i < i' \le m} n_{i}^{2}n_{i'}^{2}\\
Var[X] &=& E[X^2] - E^2[X] \le 4\sum_{1\le i < i' \le m} n_{i}^{2}n_{i'}^{2} \le 2f_2
\end{eqnarray*}
Finally, defining $Y = \frac{1}{s}\sum_{i=1}^{s}X_i$, where each $X_i$ is an independent copy of $X$ we have that:
\[
\Pr[|Y -f_2| \ge \eps f_2] \le \delta
\]
if $s \ge \frac{2}{\eps^2 \delta}$.

\section*{Connection to random projections (next class)}
  
Consider the $s$ hash functions $h_i:a\rightarrow \{-1,1\}$ we used in estimating the second frequency moment.  
Consider the matrix $H \in \R^{s \times m}$ such that $H(i,j) = h_{i}(j)$.
Also, consider representing each input element $a_i$ by $\vec{a_i}$, the $i$'th standard basis vector in $\R^m$ (the vector whose $i$'th entry is equal to $1$ and the rest are zero). Analogously, $\vec{A_i}$ is the vector representing the $i$'th element in the stream. Remember that our estimate $Y$ of $f_2$ was $\frac{1}{s}\sum_{i=1}^{s}Z_{i}^2= ||\frac{1}{\sqrt{s}}\vec{Z}||^2$. Moreover, from the definition of $\vec{Z}$, $H$, and $\vec{A_i}$ we have that $\vec{Z} = \sum_{i=1}^{N}H\vec{A_i} = H\sum_{i=1}^{N}\vec{A_i} = \frac{1}{\sqrt{s}}H\vec{A}$. Here, $\vec{A} = \sum_{i=1}^{N}\vec{A_i} = [n_1,n_2,\ldots,n_m]$. Note however, that $f_2 = ||\vec{A}||^2$ by definition of the second frequency moment.
We get that for any stream and any element frequencies $ ||\frac{1}{\sqrt{s}}H\vec{A}||^2 \approx_{(\eps,\delta)} ||\vec{A}||^2$.
In other words,  multiplying any vector $\vec{A}$ by the matrix $\frac{1}{\sqrt{s}}H$ is very likely to preserve its norm.
We will see that this phenomenon is in fact more overreaching and has some serious consequences on point ensembles in high dimensional Euclidean spaces. 

\end{document}
















%%%%%%%%
