<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
    <title dir="ltr">Edo Liberty's homepage</title>
    <meta name="keywords"
          content="Datamining Algorithms Streaming Edo Liberty SageMaker">
    <link href="style.css" rel="stylesheet" type="text/css">
</head>
<body>
	<p>
		Class created given by <a href="http://www.edoliberty.com">Edo liberty</a>.
	</p>


	<h2 dir="ltr">Data Mining</h2>
	<p>Data Mining is concerned with efficiently extracting statistics,
		patterns, structures, or meanings from raw data. This task becomes
		hard when the amount of data is large, which is often the case in
		modern data sets. This course will survey modern algorithms, concepts,
		and data structures common to data mining in large data sets. We will
		try to cover, among other topics: data sampling, finding causal
		relations and frequent item sets, counting in data streams, ranking
		and sorting, approximating large matrix operations, dimensionality
		reduction and efficient searching in high dimensions. We will also
		discuss modern cluster architectures and computational models.</p>
	<p>I recommend that students be familiar with probability theory,
		basic combinatorics, linear algebra, basic complexity theory, and
		traditional data structures, at least on at introductory level. The
		class will attempt to be self contained nonetheless.</p>


	<h2 dir="ltr">Class Details</h2>
	<p>
		The class takes place 9:00 to 12:00 in room 112 of the Dan David building. (We are no longer in Dan David 204)
	</p>
	<p>Undergraduate students:</p>
	<ul>
		<li>Final exam (50% of the grade).</li>
		<li>Home assignments (50% of the grade).</li>
		<li>A project (for extra credit) is optional for students who are
			interested.</li>
	</ul>
	<p>Master and PhD students:</p>
	<ul>
		<li>No final exam.</li>
		<li>Home assignments (50% of the grade).</li>
		<li>Final project (50% of the grade). Should require roughly a
			week's worth of work, comparable to learning for and taking the final
			exam. These project can contain both theoretical and experimental
			elements.</li>
	</ul>

	<h2 dir="ltr">Class notes</h2>
	<ul>
		<li>
			Lesson 1: Why data mining?
			<a href="pdfs/whydatamining.pdf">Presentation.</a>
		</li>
		<li>
			Lesson 1: Probability recap.
			<a href="pdfs/intro.pdf">Class notes.</a>		
		</li>
		<li>
			Lesson 1: Mark and recapture.
			<a href="pdfs/markandrecapture.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 2: Sampling.
			<a href="pdfs/sampling.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 2: The Chernoff bound.
			<a href="pdfs/chernoffbound.pdf">Class notes by John Canny)</a>
		</li>
		<li>
			Homework 1: i.i.d. sampling from streams.
			<a href="pdfs/hw1.pdf">Assignment</a>,
			<a href="pdfs/hw1_solution.pdf">solution.</a>
		</li>
		<li>
			Lesson 3: Dinamic Item Set Counting.
			<a href="pdfs/DinamicItemSetCounting.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 3: A Simple Algorithm for Finding Frequent Elements in
			Streams and Bags
			<a href="pdfs/ASimpleAlgorithmForFindingFrequentElementsInStreamsAndBags.pdf"> Class notes.</a>
		</li>
		<li>
			Lesson 4: Finding Frequent Items in Data Streams.
			<a href="pdfs/FindingFrequentItemsInDataStreams.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 4: Notes on Bloom filters vs. bit hashes.
			<a href="pdfs/bithashesAndBloomFilters.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 4: Notes on Count Sketches.
			<a href="pdfs/CountSketches.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 5: Approximating the frequency moments.
			<a href="pdfs/frequencyMoments.pdf">Class notes.</a>
		</li>
		<li>
			Homework 2: Approximate set unions.
			<a href="pdfs/hw2.pdf">Assignment</a>,
			<a href="pdfs/hw2_solution.pdf">solution.</a>
		</li>
		<li>
			Lesson 6: Random Projections.
			<a href="pdfs/randomProjections.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 7: Fast Random Projections
		</li>
		<li>
			Lesson 8: Singular Value Decomposition (SVD)
			<a href="pdfs/SingularValueDecomposition.pdf">Class notes.</a>
		</li>
		<li>
			Lesson 9: the power method.
			<a href="pdfs/ThePowerMethod.pdf"> Class notes.</a>
		</li>
		<li>
			Lesson 9: matrix sampling by Ahlswede and Winter
			<a href="pdfs/ahlswede-winter.pdf"> Paper.</a>
		</li>
		<li>
			Lesson 10: Nearest Neighbor search, KD-trees and other space
			partitioning solutions.
		</li>
		<li>
			Lesson 11: Approximate Nearest Neighbor search, Locality Sensitive Hashing
			<a href="pdfs/anns.pdf">class notes.</a>
		</li>
		<li>
			Homework 3: Document duplicate detection
			<a href="pdfs/hw3.pdf">Assignment.</a>
		</li>
		<li>
			Lesson 12: Searching, Inverted indexes and the set
			Containment problem.
			<a href="pdfs/partialMatch.pdf"> class notes.</a>
		<li>
			Lesson 13: Material review. 
			<a href="pdfs/exampleTestQuestions.pdf">Example test questions.</a>
		</li>
		<li>
			Lesson 14: Introduction to spectral graph theory
			<a href="pdfs/spectralGraphTheory1.pdf">Class notes 1</a>,
			<a href="pdfs/spectralGraphTheory2.pdf">class notes 2</a>, and
			<a href="pdfs/spectralGraphTheory3.pdf">class notes 3</a>. All by Daniel A. Spielman.
		</li>
		<li>
			Exam 1: <a href="pdfs/test_alef.pdf">"Moed Alef"</a>
		</li>
		<li>
			Exam 1: <a href="pdfs/test_alef_answers.pdf">"Moed Alef" answers.</a>
		</li>
		<li>	
			Exam 2: <a href="pdfs/test_beit.pdf">"Moed Beit"</a>
		</li>
	</ul>


	<script>
		(function(i, s, o, g, r, a, m) {
			i['GoogleAnalyticsObject'] = r;
			i[r] = i[r] || function() {
				(i[r].q = i[r].q || []).push(arguments)
			}, i[r].l = 1 * new Date();
			a = s.createElement(o), m = s.getElementsByTagName(o)[0];
			a.async = 1;
			a.src = g;
			m.parentNode.insertBefore(a, m)
		})(window, document, 'script',
				'//www.google-analytics.com/analytics.js', 'ga');

		ga('create', 'UA-69545895-1', 'auto');
		ga('send', 'pageview');
	</script>
</body>
</html>
